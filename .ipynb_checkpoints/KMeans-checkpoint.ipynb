{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as scp\n",
    "\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-5-58fda05fe307>, line 95)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-58fda05fe307>\"\u001b[0;36m, line \u001b[0;32m95\u001b[0m\n\u001b[0;31m    def get_bic_score(self, X):\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "class KMeans():\n",
    "    def __init__(self):\n",
    "        self.centroids = []\n",
    "        self.distance_function = None\n",
    "        self.max_iterations = 0\n",
    "        self.min_adjustment = 0\n",
    "    \n",
    "    \n",
    "    def with_euclidian_distance(self):\n",
    "        self.distance_function = lambda x, y: np.linalg.norm(np.array(x) - np.array(y), axis=0)\n",
    "        return self\n",
    "    \n",
    "    def with_cosin_similarity(self):\n",
    "        self.distance_function = lambda x, y: scp.spatial.distance.cosine(np.array(x), np.array(y))\n",
    "        return self\n",
    "    \n",
    "    def set_max_iterations(self, iterations):\n",
    "        self.max_iterations = iterations\n",
    "        return self\n",
    "    \n",
    "    def set_min_adjustment(self, adjustment):\n",
    "        self.min_adjustment = adjustment\n",
    "        return self\n",
    "    \n",
    "    def initialize_random(self, centroid_number, data):\n",
    "        data_count, dimensions = data.shape\n",
    "        selected_rows = np.random.randint(0, data_count, centroid_number)\n",
    "        self.centroids = data[selected_rows]\n",
    "        return self\n",
    "    \n",
    "    def initialize_plus_plus(self, centroid_number, data):\n",
    "        data_count, dimensions = data.shape\n",
    "        self.centroids = [data[np.random.randint(0, data_count)]]\n",
    "        \n",
    "        for _ in range(centroid_number - 1):\n",
    "            squared_distances_to_bmus = np.array([self.distance_function(x, self.get_bmu(x)[1]) ** 2 for x in data])\n",
    "            probabilities = squared_distances_to_bmus / squared_distances_to_bmus.sum()\n",
    "            probabilites_accumulated = probabilities.cumsum()\n",
    "            \n",
    "            r = np.random.random()\n",
    "            ind = np.where(probabilites_accumulated >= r)[0][0]\n",
    "            self.centroids.append(data[ind])\n",
    "        return self\n",
    "\n",
    "    def train(self, X):\n",
    "        finished = False\n",
    "        current_iteration = 0\n",
    "        while not finished:\n",
    "            adjustment = self.fit(X)\n",
    "            current_iteration += 1\n",
    "            if self.max_iterations > 0 and current_iteration > self.max_iterations:\n",
    "                finished = True\n",
    "\n",
    "            if adjustment <= self.min_adjustment:\n",
    "                finished = True\n",
    "\n",
    "        return self, current_iteration\n",
    "    \n",
    "    def predict(self, x):\n",
    "        group_id, bmu = self.get_bmu(x)\n",
    "        return group_id, bmu\n",
    "\n",
    "    def get_bmu(self, x):\n",
    "        bmu_id, bmu, bmu_dist = 0, self.centroids[0], self.distance_function(self.centroids[0], x)\n",
    "        for i, c in enumerate(self.centroids[1:], 1):\n",
    "            c_dist = self.distance_function(c, x)\n",
    "            if c_dist < bmu_dist:\n",
    "                bmu_id, bmu, bmu_dist = i, c, c_dist\n",
    "        return bmu_id, bmu\n",
    "\n",
    "    def fit(self, X):\n",
    "        groups = [[] for i in self.centroids]\n",
    "        for x in X:\n",
    "            bmu_id, _ = self.get_bmu(x)\n",
    "            groups[bmu_id].append(x)\n",
    "        \n",
    "        adjustment = 0\n",
    "        for group_id, _ in enumerate(groups):\n",
    "            group_mean = np.mean(groups[group_id], 0) if len(groups[group_id]) > 0 else None\n",
    "            if group_mean is not None:\n",
    "                adjustment += self.distance_function(self.centroids[group_id], group_mean)\n",
    "                self.centroids[group_id] = group_mean\n",
    "        \n",
    "        self.groups = groups\n",
    "        return adjustment\n",
    "    \n",
    "    def get_squared_mean_error(self, X):\n",
    "        error = 0\n",
    "        for x in X:\n",
    "            error += self.distance_function(x, self.get_bmu(x)[1]) ** 2\n",
    "        return error\n",
    "    \n",
    "    def get_bic_score(self, X):\n",
    "        \n",
    "        # assign centers and labels\n",
    "        centers = self.centroids\n",
    "        #number of clusters\n",
    "        m = len(self.centroids)\n",
    "        # size of the clusters\n",
    "        n = [len(g) for g in self.groups]\n",
    "\n",
    "        data_count, dimensions = X.shape\n",
    "        \n",
    "        bic_score = self.get_squared_mean_error(X) + len(centers) * dimensions * np.log(data_count) / 2\n",
    "        return bic_score\n",
    "    \n",
    "    def get_sillhouete_score(self, X):\n",
    "        self.groups = [[] for i in self.centroids]\n",
    "        for i, x in enumerate(X):\n",
    "            bmu_id, _ = self.get_bmu(x)\n",
    "            self.groups[bmu_id].append({'i': i, 'x': x})\n",
    "\n",
    "        indexes = []\n",
    "        for i, group in enumerate(self.groups):\n",
    "            s_group = []\n",
    "            indexes.append(s_group)\n",
    "            other_clusters = np.delete(self.groups, i, axis=0)\n",
    "            for x_i, x_tuple in enumerate(group):\n",
    "                x_identifier, x = x_tuple['i'], x_tuple['x']\n",
    "                group_without_x = np.delete(group, x_i, axis=0)\n",
    "                average_intra_cluster_distance = np.mean([self.distance_function(x, y['x']) for y in group_without_x])\n",
    "                \n",
    "                neighbor_clusters_medium_distances=[np.mean([self.distance_function(x, y['x']) for y in neighbor]) for neighbor in other_clusters]\n",
    "                min_average_extra_cluster_distance = np.amin(neighbor_clusters_medium_distances)\n",
    "                \n",
    "                x_score = (min_average_extra_cluster_distance - average_intra_cluster_distance) / max(min_average_extra_cluster_distance, average_intra_cluster_distance)\n",
    "                s_group.append((x_identifier, x_score))\n",
    "        return indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "class XMeans(KMeans):\n",
    "    \n",
    "    def create_k_means_copy(self):\n",
    "        instance = KMeans()\n",
    "        instance.centroids = copy.deepcopy(self.centroids)\n",
    "        instance.distance_function = self.distance_function\n",
    "        instance.max_iterations = self.max_iterations\n",
    "        instance.min_adjustment = self.min_adjustment\n",
    "        return instance\n",
    "    \n",
    "    def set_centroid_estimation_range(self, minimum, maximum):\n",
    "        self.minimum = minimum\n",
    "        self.maximum = maximum\n",
    "        return self\n",
    "    \n",
    "    def train(self, X):\n",
    "        self.initialize_plus_plus(self.minimum, X)\n",
    "\n",
    "        super().train(X)\n",
    "        \n",
    "        while len(self.centroids) < self.maximum:\n",
    "            old_len = len(self.centroids)\n",
    "            self.centroids = self.plan_new_centroids(X)\n",
    "            \n",
    "            super().train(X)\n",
    "            if len(self.centroids) == old_len:\n",
    "                break\n",
    "        return self, len(self.centroids)\n",
    "    \n",
    "    def plan_new_centroids(self, X):\n",
    "\n",
    "        new_centroids = []\n",
    "        for i, old_centroid in enumerate(self.centroids):\n",
    "            if len(new_centroids) >= self.maximum - 1:\n",
    "                break\n",
    "                \n",
    "            pre_split_kmeans = self.create_k_means_copy()\n",
    "            hipotesis_kmeans = self.create_k_means_copy()\n",
    "    \n",
    "            x_in_centroid_group = np.array([x for x in X if hipotesis_kmeans.predict(x)[0] == i])\n",
    "            \n",
    "            pre_split_kmeans.centroids = [old_centroid]\n",
    "            \n",
    "            data_count, _ = x_in_centroid_group.shape\n",
    "            delta_vector = x_in_centroid_group[np.random.randint(0, data_count)]\n",
    "            \n",
    "            hipotesis_kmeans.centroids = [delta_vector, 2 * np.array(old_centroid) - np.array(delta_vector)]\n",
    "            \n",
    "            pre_split_kmeans.train(x_in_centroid_group)\n",
    "            hipotesis_kmeans.train(x_in_centroid_group)\n",
    "            \n",
    "            # Compare pre_split and post_split models\n",
    "            if hipotesis_kmeans.get_bic_score(X) < pre_split_kmeans.get_bic_score(X):\n",
    "                new_centroids += hipotesis_kmeans.centroids\n",
    "            else:\n",
    "                new_centroids += pre_split_kmeans.centroids\n",
    "        return new_centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path):\n",
    "    x = np.genfromtxt(path, delimiter=',', skip_header=1)\n",
    "    x = np.delete(x, 0, axis=1)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_data('data/bbc/BsaidaBin98.csv')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn import cluster, datasets, mixture\n",
    "from itertools import cycle, islice\n",
    "%matplotlib inline\n",
    "np.random.seed(555)\n",
    "blobs = datasets.make_blobs(n_samples=50, centers=5)\n",
    "#blobs = np.random.random((50, 2)) * 10, 1\n",
    "\n",
    "\n",
    "X, y = blobs\n",
    "\n",
    "def test_instance(instance, X=X, y=y):\n",
    "    fig = plt.figure(figsize=(5, 5))\n",
    "    predictions = [instance.predict(x) for x in X]\n",
    "    y_pred = [y for y, _ in predictions]\n",
    "\n",
    "    colors = np.array(list(islice(cycle(['#377eb8', '#ff7f00', '#4daf4a',\n",
    "                                                 '#f781bf', '#a65628', '#984ea3',\n",
    "                                                 '#999999', '#e41a1c', '#dede00']),\n",
    "                                          int(max(y_pred) + 1))))\n",
    "\n",
    "    plt.scatter(X[:, 0], X[:, 1], color=colors[y_pred])\n",
    "    for i, centroid in enumerate(instance.centroids):\n",
    "        square = plt.Rectangle(centroid, 0.21, 0.2, color='black')\n",
    "        fig.gca().add_artist(square)\n",
    "\n",
    "    sillhouetes = np.array([[i, val, g] for g, group in enumerate(instance.get_sillhouete_score(X)) for i, val in group ])\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.barh(sillhouetes[:, 0].astype(str),sillhouetes[:, 1], align='center',\n",
    "            color=colors[sillhouetes[:, 2].astype(int)], ecolor='black')\n",
    "    #ax.set_yticks(y_pos)\n",
    "    ax.set_yticklabels([])\n",
    "    #ax.invert_yaxis()  # labels read top-to-bottom\n",
    "    ax.set_xlabel('S Score')\n",
    "    ax.set_title('S Score')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'KMeans' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-cdedd0388414>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mkmeans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m             \u001b[0;34m.\u001b[0m\u001b[0minitialize_random\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m             \u001b[0;34m.\u001b[0m\u001b[0mwith_euclidian_distance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m             \u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Trained in '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'iterations'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtest_instance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkmeans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'KMeans' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "kmeans, iterations = KMeans() \\\n",
    "            .initialize_random(5, X) \\\n",
    "            .with_euclidian_distance() \\\n",
    "            .train(X)\n",
    "            \n",
    "print('Trained in ', iterations, 'iterations')\n",
    "test_instance(kmeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans, iterations = KMeans() \\\n",
    "            .with_euclidian_distance() \\\n",
    "            .initialize_plus_plus(5, X) \\\n",
    "            .train(X)\n",
    "            \n",
    "print('Trained in ', iterations, 'iterations')\n",
    "test_instance(kmeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "kmeans, iterations = XMeans() \\\n",
    "            .with_euclidian_distance() \\\n",
    "            .set_centroid_estimation_range(1, 5) \\\n",
    "            .train(X)\n",
    "            \n",
    "print('Created', iterations, 'centroids')\n",
    "    \n",
    "test_instance(kmeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "262.4076418951341"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans.get_squared_mean_error(X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
